{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Memory, and Human-in-the-Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll cover a series of topics that are important to take your application from running a notebook to running in production. Here are the topics\n",
    "\n",
    "1. Streaming: How does streaming work with LangGraph?\n",
    "2. Memory: How can persist information in our app across invocations?\n",
    "    - Local memory\n",
    "    - External memory\n",
    "3. Breakpoints and Human-in-the-loop\n",
    "\n",
    "After this section, we'll take a closer look at memory and HIL in LangGraph Studio, as well as production monitoring in LangSmith."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our graph is getting pretty big and complex! Let's copy over what we've done through the first three modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from utils import get_vector_db_retriever, RAG_PROMPT\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import Document\n",
    "from typing import List\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.constants import Send\n",
    "from collections import Counter\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph import START, END\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(dotenv_path=\"../.env\", override=True)\n",
    "\n",
    "# Fetch retriever\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "# Set up LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Define our GraphState, InputState, and OutputState\n",
    "def custom_documents_reducer(existing, update):\n",
    "    # If we passed in a dictionary that asks for \"overwrite\", then we return the updated documents only\n",
    "    if isinstance(update, dict) and update[\"type\"] == \"overwrite\":\n",
    "        return update[\"documents\"]\n",
    "\n",
    "    # Otherwise, we simple add the lists\n",
    "    return existing + update\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    rewritten_queries: List[str]\n",
    "    generation: str\n",
    "    documents: Annotated[List[Document], custom_documents_reducer]   # We use Annotated to add our custom reducer\n",
    "    attempted_generations: int\n",
    "class InputState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the input state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "    \"\"\"\n",
    "    question: str\n",
    "class OutputState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the final outputstate of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "    generation: str\n",
    "    documents: List[Document]\n",
    "\n",
    "# Define Nodes and Conditional Edges\n",
    "def retrieve_documents(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): A dictionary containing a question\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    sample_answer_prompt_formatted = sample_answer_prompt.format(question=question)\n",
    "    response = sample_answer_llm.invoke(\n",
    "        [SystemMessage(content=sample_answer_system_prompt)] + [HumanMessage(content=sample_answer_prompt_formatted)]\n",
    "    )\n",
    "    sample_answer = response.sample_answer\n",
    "    documents = retriever.invoke(f\"{question}: {sample_answer}\")    # Now we use our question and sample answer\n",
    "    return {\"documents\": documents}\n",
    "def generate_response(state: GraphState):\n",
    "    \"\"\"\n",
    "    Generate response\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE RESPONSE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    attempted_generations = state.get(\"attempted_generations\", 0)   # By default we set attempted_generations to 0 if it doesn't exist yet\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    \n",
    "    # RAG generation\n",
    "    rag_prompt_formatted = RAG_PROMPT.format(context=formatted_docs, question=question)\n",
    "    generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n",
    "    return {\n",
    "        \"documents\": documents,\n",
    "        \"question\": question,\n",
    "        \"generation\": generation,\n",
    "        \"attempted_generations\": attempted_generations + 1   # In our state update, we increment attempted_generations\n",
    "    }\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "grade_documents_llm = llm.with_structured_output(GradeDocuments)\n",
    "grade_documents_system_prompt = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "grade_documents_prompt = \"Here is the retrieved document: \\n\\n {document} \\n\\n Here is the user question: \\n\\n {question}\"\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # -- New logic to deduplicate documents our queries --\n",
    "    doc_counter = Counter(doc.page_content for doc in documents)\n",
    "    most_common_contents = doc_counter.most_common(5)\n",
    "    top_documents = []\n",
    "    for content, _ in most_common_contents:\n",
    "        for d in documents:\n",
    "            if d.page_content == content:\n",
    "                top_documents.append(d)\n",
    "                break\n",
    "\n",
    "    # Score each one of our five most common documents\n",
    "    filtered_docs = []\n",
    "    for d in top_documents:\n",
    "        grade_documents_prompt_formatted = grade_documents_prompt.format(document=d.page_content, question=question)\n",
    "        score = grade_documents_llm.invoke(\n",
    "            [SystemMessage(content=grade_documents_system_prompt)] + [HumanMessage(content=grade_documents_prompt_formatted)]\n",
    "        )\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            continue\n",
    "    return {\"documents\": {\"type\": \"overwrite\", \"documents\": filtered_docs}, \"question\": question}\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or to terminate execution.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    state[\"question\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, END---\"\n",
    "        )\n",
    "        return \"none relevant\"    # same as END\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"some relevant\" \n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )\n",
    "grade_hallucinations_llm = llm.with_structured_output(GradeHallucinations)\n",
    "grade_hallucinations_system_prompt = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n \n",
    "     Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
    "grade_hallucinations_prompt = \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"\n",
    "ATTEMPTED_GENERATION_MAX = 3\n",
    "def grade_hallucinations(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "    attempted_generations = state[\"attempted_generations\"]\n",
    "\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "\n",
    "    grade_hallucinations_prompt_formatted = grade_hallucinations_prompt.format(\n",
    "        documents=formatted_docs,\n",
    "        generation=generation\n",
    "    )\n",
    "\n",
    "    score = grade_hallucinations_llm.invoke(\n",
    "        [SystemMessage(content=grade_hallucinations_system_prompt)] + [HumanMessage(content=grade_hallucinations_prompt_formatted)]\n",
    "    )\n",
    "    grade = score.binary_score\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        return \"supported\"\n",
    "    elif attempted_generations >= ATTEMPTED_GENERATION_MAX:    # New condition!\n",
    "        print(\"---DECISION: TOO MANY ATTEMPTS, GIVE UP---\")\n",
    "        raise RuntimeError(\"Too many attempted generations with hallucinations, giving up.\")\n",
    "        # return \"give up\"    # Note: We could also do this to silently fail\n",
    "    else:\n",
    "        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\"\n",
    "class RewrittenQueries(BaseModel):\n",
    "    \"\"\"Rewritten queries based on the user's original question.\"\"\"\n",
    "    rewritten_queries: List[str] = Field(\n",
    "        description=\"A list of rewritten versions of the user's query. Each rewritten version is rewritten differently, rephrased and potentially uses synonyms.\"\n",
    "    )\n",
    "rewritten_query_llm = llm.with_structured_output(RewrittenQueries)\n",
    "rewritten_query_system_prompt = \"\"\"You are an analyst in charge of taking a user's question as input, and reframing and rewriting it in different ways.\\n\n",
    "Your goal is to change the phrasing of the question, while making sure that the intent and meaning of the question is the same.\\n\n",
    "Return a list of rewritten_queries. The number will be specified by the user.\"\"\"\n",
    "rewritten_query_prompt = \"Here is the user's question: \\n\\n {question}. Return {num_rewrites} queries.\"\n",
    "class SampleAnswer(BaseModel):\n",
    "    \"\"\"Sample answer for an input question.\"\"\"\n",
    "    sample_answer: str = Field(\n",
    "        description=\"A concise example answer for a question. This shouldn't exceed three sentences in length.\"\n",
    "    )\n",
    "sample_answer_llm = llm.with_structured_output(SampleAnswer)\n",
    "sample_answer_system_prompt = \"\"\"You are a novice in charge of taking a user's question as input, and generating a sample answer for it.\\n\n",
    "This sample answer should contain words that would likely be in a real answer, but is not grounded in any factual documents, the way a real answer would be.\"\"\"\n",
    "sample_answer_prompt = \"Here is the user's question: \\n\\n {question}.\"\n",
    "def generate_rewritten_queries(state):\n",
    "    \"\"\"\n",
    "    Generates rewritten versions of the original user query\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates rewritten_queries key with a list of rewritten queries\n",
    "    \"\"\"\n",
    "    print(\"---GENERATING REWRITTEN VERSIONS OF THE USER'S QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "    num_rewrites = 3\n",
    "    rewritten_query_prompt_formatted = rewritten_query_prompt.format(question=question, num_rewrites=num_rewrites)\n",
    "    response = rewritten_query_llm.invoke(\n",
    "        [SystemMessage(content=rewritten_query_system_prompt)] + [HumanMessage(content=rewritten_query_prompt_formatted)]\n",
    "    )\n",
    "    rewritten_queries = response.rewritten_queries\n",
    "\n",
    "    return {\"rewritten_queries\": rewritten_queries}\n",
    "def continue_to_retrieval_nodes(state: GraphState):\n",
    "    edges_to_create = []\n",
    "    # Add original question\n",
    "    edges_to_create.append(Send(\"retrieve_documents\", {\"question\": state[\"question\"]}))\n",
    "    # Add rewritten queries\n",
    "    for rewritten_query in state[\"rewritten_queries\"]:\n",
    "        edges_to_create.append(Send(\"retrieve_documents\", {\"question\": rewritten_query}))\n",
    "    return edges_to_create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangGraph is built with [first class support for streaming](https://langchain-ai.github.io/langgraph/concepts/low_level/#streaming)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's talk about ways to [stream our graph state](https://langchain-ai.github.io/langgraph/concepts/low_level/#streaming).\n",
    "\n",
    "`.stream` and `.astream` are sync and async methods for streaming back results. \n",
    " \n",
    "LangGraph supports a few [different streaming modes](https://langchain-ai.github.io/langgraph/how-tos/stream-values/) for [graph state](https://langchain-ai.github.io/langgraph/how-tos/stream-values/):\n",
    " \n",
    "* `values`: This streams the full state of the graph after each node is called.\n",
    "* `updates`: This streams updates to the state of the graph after each node is called.\n",
    "\n",
    "![values_vs_updates.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbaf892d24625a201744e5_streaming1.png)\n",
    "\n",
    "Let's look at `stream_mode=\"updates\"`.\n",
    "\n",
    "Because we stream with `updates`, we only see updates to the state after node in the graph is run.\n",
    "\n",
    "Each `chunk` is a dict with `node_name` as the key and the updated state as the value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakpoints and Human-in-the-loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lg-101-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
